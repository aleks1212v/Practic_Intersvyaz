{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Doc2Vec_training_dbow.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"JJFi9mwZwTxc","colab":{"base_uri":"https://localhost:8080/","height":560},"executionInfo":{"status":"error","timestamp":1595576466106,"user_tz":-300,"elapsed":60780,"user":{"displayName":"Алексей Витомсков","photoUrl":"","userId":"09837648934599257287"}},"outputId":"21535db8-79f3-4d36-e1ea-0b803ae9e867"},"source":["#DBOW DOC2VEC model\n","\n","import logging\n","import pymysql\n","import multiprocessing\n","import gensim\n","from gensim.models import Doc2Vec\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","from multiprocessing import cpu_count\n","cpu_count = cpu_count()\n","print(f\"Количество потоков: {cpu_count}\")\n","\n","import numpy as np\n","\n","# Connect to the database\n","def get_connection():\n","    connection = pymysql.connect(host='database-3.chim8btj05zt.us-east-1.rds.amazonaws.com',\n","                                 user='admin',\n","                                 password='01035007',\n","                                 db='test1',\n","                                 charset='utf8mb4',\n","                                 cursorclass=pymysql.cursors.DictCursor)\n","    return connection\n","\n","# Create the tagged document needed for Doc2Vec\n","def create_tagged_document(list_of_list_of_words):\n","    for i, list_of_words in enumerate(list_of_list_of_words):\n","        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n","\n","\n","connection = get_connection()\n","\n","EPOCHS_0 = 20 #эпохи первичное обучение\n","EPOCHS_1 = 20 #эпохи дообучение\n","COUNT_DB = 10314497 #число строк в обработанной БД\n","LIMIT_0 = 2000000 #загружаем столько строк для первичного обучения модели\n","LIMIT_1 = LIMIT_0  #загружаем столько строк для последующего обучения модели\n","\n","SIZE = 1000 #размерность выходного вектора признаков\n","ITER = 1 #C какой итерации дообучать (0 - первичное обучение)\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","try:\n","    with connection.cursor() as cursor:\n","        if ITER == 0:\n","            #вычисление конечного id для ITER_0\n","            s = \"SELECT max(id) FROM (SELECT id from sentiment_v3 ORDER BY id ASC LIMIT %s) AS t\"\n","            cursor.execute(s, (LIMIT_0))\n","            end_id = cursor.fetchone()['max(id)']\n","            print(end_id)\n","\n","            #вычисление максимального id для всей БД\n","            s = \"SELECT max(id) FROM sentiment_v3\"\n","            cursor.execute(s)\n","            max_id = cursor.fetchone()['max(id)']\n","\n","            #загрузка данных для iter_0\n","            s = \"SELECT ttext_stem FROM sentiment_v3 ORDER BY id ASC LIMIT %s\"\n","            cursor.execute(s, (LIMIT_0))\n","            result = cursor.fetchall()\n","            result = [doc['ttext_stem'].split('|') for doc in result]\n","            print(f\"Считано {len(result)} строк из базы данных\")\n","\n","            train_data = list(create_tagged_document(result))\n","            result = None\n","\n","            #instantiate our DM and DBOW models\n","            model = Doc2Vec(min_count=5, window=5,  vector_size=SIZE, sample=1e-3, negative=5, dm=0, workers=cpu_count, dbow_words=0) #dm = 0  DBOW_model\n","            \n","            #build vocab over all reviews\n","            model.build_vocab(train_data, update = False)\n","\n","            #train\n","            model.train(documents=train_data, epochs=EPOCHS_0, total_examples=model.corpus_count)\n","            model.save(\"/gdrive/My Drive/Colab Notebooks/d2v_dbow_1000/model_dbow1000_iter_0.doc2vec\", sep_limit = 100 * 1024 ** 2)\n","            #model.delete_temporary_training_data()\n","          \n","            print(f\"Модель model_dm записана на гугл диск\")\n","            COUNT_DB -= LIMIT_0\n","            print(f\"Осталось обработать {COUNT_DB} записей!\")\n","            ITER += 1\n","\n","        else:\n","            #вычисление конечного id для ITER_X\n","            s = \"SELECT max(id) FROM (SELECT id from sentiment_v3 ORDER BY id ASC LIMIT %s) AS t\"\n","            cursor.execute(s, (LIMIT_0 + LIMIT_1 * (ITER - 1)))\n","            end_id = cursor.fetchone()['max(id)']\n","            print(end_id)\n","\n","            #вычисление максимального id для всей БД\n","            s = \"SELECT max(id) FROM sentiment_v3\"\n","            cursor.execute(s)\n","            max_id = cursor.fetchone()['max(id)']\n","\n","\n","\n","        \n","        # дообучение\n","        while end_id < max_id:\n","            \n","            print(f\"Итерация {ITER}:\")\n","            model_old_pach = \"/gdrive/My Drive/Colab Notebooks/d2v_dbow_1000/model_dbow1000_iter_\" + str(ITER - 1) + \".doc2vec\"\n","            model_pach = \"/gdrive/My Drive/Colab Notebooks/d2v_dbow_1000/model_dbow1000_iter_\" + str(ITER) + \".doc2vec\"\n","\n","\n","            #загрузка данных для iter_X\n","            s = \"SELECT ttext_stem FROM sentiment_v3 WHERE id > %s ORDER BY id ASC LIMIT %s\"\n","            cursor.execute(s, (end_id, LIMIT_1))\n","            result = cursor.fetchall()\n","            result = [doc['ttext_stem'].split('|') for doc in result]\n","            print(f\"Считано {len(result)} строк из базы данных\")\n","\n","            #вычисление конечного id для ITER_X\n","            s = \"SELECT max(id) FROM (SELECT id FROM sentiment_v3 WHERE id > %s ORDER BY id ASC LIMIT %s) AS t\"\n","            cursor.execute(s, (end_id, LIMIT_1))\n","            end_id = cursor.fetchone()['max(id)']\n","\n","            train_data = list(create_tagged_document(result))\n","            result = None\n","\n","            #instantiate our DM and DBOW models\n","\n","            model = Doc2Vec.load(model_old_pach) #, mmap='r')  \n","            \n","            #build vocab over all reviews\n","            model.build_vocab(train_data, update = True)\n","            \n","            #train\n","            model.train(documents=train_data, epochs=EPOCHS_1, total_examples=model.corpus_count)\n","\n","            #model.delete_temporary_training_data()\n","\n","            model.save(model_pach, sep_limit = 100 * 1024 ** 2)\n","            print(f\"Модель записана на гугл диск\")\n","\n","            COUNT_DB -= LIMIT_1\n","            print(f\"Осталось обработать {COUNT_DB} записей!\")\n","\n","            ITER += 1\n","finally:\n","    connection.close()\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","Количество потоков: 2\n","411050803860733952\n","Итерация 1:\n","Считано 2000000 строк из базы данных\n"],"name":"stdout"},{"output_type":"stream","text":["2020-07-24 07:41:05,451 : INFO : loading Doc2Vec object from /gdrive/My Drive/Colab Notebooks/d2v_dbow_1000/model_dbow1000_iter_0.doc2vec\n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"error","ename":"EOFError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c32360c5b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m#instantiate our DM and DBOW models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_old_pach\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, mmap='r')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m#build vocab over all reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \"\"\"\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Ran out of input"]}]},{"cell_type":"code","metadata":{"id":"GMla2Y4N526k","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1595576357647,"user_tz":-300,"elapsed":96650,"user":{"displayName":"Алексей Витомсков","photoUrl":"","userId":"09837648934599257287"}}},"source":["#Обновление модели doc2vec\n","import logging\n","import pymysql\n","import multiprocessing\n","import gensim\n","from gensim.models import Doc2Vec\n","\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","from multiprocessing import cpu_count\n","cpu_count = cpu_count()\n","print(f\"Количество потоков: {cpu_count}\")\n","\n","import numpy as np\n","\n","# Connect to the database\n","def get_connection():\n","    connection = pymysql.connect(host='database-3.chim8btj05zt.us-east-1.rds.amazonaws.com',\n","                                 user='admin',\n","                                 password='01035007',\n","                                 db='test1',\n","                                 charset='utf8mb4',\n","                                 cursorclass=pymysql.cursors.DictCursor)\n","    return connection\n","\n","# Create the tagged document needed for Doc2Vec\n","def create_tagged_document(list_of_list_of_words):\n","    for i, list_of_words in enumerate(list_of_list_of_words):\n","        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n","\n","\n","connection = get_connection()\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","try:\n","    with connection.cursor() as cursor:\n","\n","        \n","  \n","\n","finally:\n","    connection.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8hkYjEktSlf","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1595576357650,"user_tz":-300,"elapsed":96649,"user":{"displayName":"Алексей Витомсков","photoUrl":"","userId":"09837648934599257287"}}},"source":["#Предполагаемый вектор абзаца для нового документа.\n","print(model_dm.infer_vector(['привет', 'я', 'пришел'], epochs=100)) \n","print(model_dbow.infer_vector(['привет', 'я', 'пришел']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sEPM8Ulu_rel","colab_type":"text"},"source":["Параметры модели doc2vec\n","\n","документы (итерируемый из списка TaggedDocument, необязательный) - входной корпус может быть просто списком элементов, но для больших корпораций рассмотрим итеративный, который передает документы непосредственно с диска / сети. Если вы не предоставите документы (или файл corpus_file ), модель останется неинициализированной - используйте, если вы планируете инициализировать ее каким-либо другим способом.\n","\n","corpus_file ( str , необязательно ) - путь к файлу корпуса в LineSentenceформате. Вы можете использовать этот аргумент вместо документов, чтобы повысить производительность. Необходимо передать только один из документов или аргументы corpus_file (или ни один из них, в этом случае модель не остается инициализированной). Теги документов назначаются автоматически и равны номеру строки, как в TaggedLineDocument.\n","\n","dm ( {1 , 0} , необязательно ) - определяет алгоритм обучения. Если dm = 1 , используется «распределенная память» (PV-DM). В противном случае используется распределенный пакет слов (PV-DBOW).\n","\n","vector_size ( int , необязательный ) - размерность векторов объектов.\n","\n","window ( int , необязательный ) - максимальное расстояние между текущим и прогнозируемым словом в предложении.\n","\n","альфа (с плавающей точкой , необязательно ) - начальная скорость обучения.\n","\n","min_alpha ( float , необязательно ) - скорость обучения будет линейно снижаться до min_alpha в процессе обучения.\n","\n","seed ( int , необязательный ) - Seed для генератора случайных чисел. Начальные векторы для каждого слова засеяны хешем конкатенации слова + str (семя) . Обратите внимание, что для полностью детерминированно-воспроизводимого прогона вы также должны ограничить модель одним рабочим потоком (worker = 1 ), чтобы исключить дрожание порядка в планировании потоков ОС. В Python 3 воспроизводимость между запусками интерпретатора также требует использования переменной среды PYTHONHASHSEED для управления рандомизацией хеша.\n","\n","min_count ( int , необязательный ) - игнорирует все слова с общей частотой ниже этой.\n","\n","max_vocab_size ( int , необязательный ) - ограничивает ОЗУ при построении словарного запаса; если есть больше уникальных слов, чем это, то удалите редкие слова. Каждые 10 миллионов типов слов требуют около 1 ГБ оперативной памяти. Установите Нет для отсутствия ограничений.\n","\n","sample ( float , необязательный ) - порог для настройки того, какие высокочастотные слова случайным образом отбираются, полезный диапазон (0, 1e-5).\n","\n","working ( int , необязательный ) - используйте эти многочисленные рабочие потоки для обучения модели (= более быстрое обучение на многоядерных машинах).\n","\n","epochs ( int , необязательный ) - количество итераций (эпох) по всему корпусу.\n","\n","hs ( {1 , 0} , необязательно ) - если 1, для обучения модели будет использоваться иерархический softmax. Если установлено значение 0, а отрицательное значение не равно нулю, будет использоваться отрицательная выборка.\n","\n","отрицательный ( int , необязательный ) - если> 0, будет использоваться отрицательная выборка, int для отрицательного указывает, сколько «шумовых слов» следует нарисовать (обычно между 5-20). Если установлено значение 0, отрицательная выборка не используется.\n","\n","ns_exponent ( float , необязательный ) - показатель степени, используемый для формирования отрицательного распределения выборки. Значение 1,0 выборки точно пропорционально частотам, 0,0 выборки всех слов одинаково, в то время как отрицательное значение выборки низкочастотных слов больше, чем высокочастотных слов. Популярное значение по умолчанию 0,75 было выбрано оригинальной статьей Word2Vec. Совсем недавно, в https://arxiv.org/abs/1804.04212 , Caselles-Dupré, Lesaint и Royo-Letelier предлагают, чтобы другие значения могли работать лучше для рекомендательных приложений.\n","\n","dm_mean ( {1 , 0} , необязательно ) - если 0, используйте сумму векторов слова контекста. Если 1, используйте среднее. Применяется только когда dm используется в неконкатентивном режиме.\n","\n","dm_concat ( {1 , 0} , необязательно ) - если 1, использовать конкатенацию векторов контекста, а не сумму / среднее; Обратите внимание, что конкатенация приводит к гораздо большей модели, так как вход больше не является размером одного (сэмплированного или арифметически сложенного) слова-вектора, а размером тега (-ов) и всех слов в контексте, связанных вместе.\n","\n","dm_tag_count ( int , необязательный ) - ожидаемое постоянное количество тегов документа на документ при использовании режима dm_concat.\n","\n","dbow_words ( {1 , 0} , необязательно ) - если установлено значение 1, обучает векторы слов (в режиме пропуска граммы) одновременно с обучением doc-vector DBOW; Если 0, обучаются только векторы документов (быстрее).\n","\n","trim_rule ( функция , необязательно ) -\n","\n","Правило обрезки словаря, указывает, должны ли определенные слова оставаться в словаре, обрезаться или обрабатываться по умолчанию (отбрасывать, если количество слов <min_count). Может быть None (будет использоваться min_count, смотрите keep_vocab_item()) или вызываться, который принимает параметры (word, count, min_count) и возвращает либо gensim.utils.RULE_DISCARD, gensim.utils.RULE_KEEPлибо gensim.utils.RULE_DEFAULT. Правило, если оно задано, используется только для сокращения словарного запаса во время текущего вызова метода и не сохраняется как часть модели.\n","\n","Входные параметры имеют следующие типы:\n","слово (str) - слово, которое мы изучаем\n","\n","count (int) - частота слова в корпусе\n","\n","min_count (int) - минимальный порог счета.\n","\n","обратные вызовы - список обратных вызовов, которые необходимо выполнить / запустить на определенных этапах во время обучения."]}]}